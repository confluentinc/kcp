package metrics

import (
	"fmt"
	"log/slog"
	"strings"
	"time"

	"github.com/confluentinc/kcp/internal/build_info"
	"github.com/confluentinc/kcp/internal/services/markdown"
	"github.com/confluentinc/kcp/internal/types"
	"github.com/confluentinc/kcp/internal/utils"
)

type ReportService interface {
	ProcessState(state types.State) types.ProcessedState
	FilterClusterMetrics(processedState types.ProcessedState, clusterArn string, startTime, endTime *time.Time) (*types.ProcessedClusterMetrics, error)
}

type MetricReporterOpts struct {
	ClusterArns []string
	State       *types.State
	StartDate   *time.Time
	EndDate     *time.Time
}

type MetricReporter struct {
	reportService ReportService

	clusterArns []string
	state       *types.State
	startDate   *time.Time
	endDate     *time.Time
}

func NewMetricReporter(reportService ReportService, opts MetricReporterOpts) *MetricReporter {
	return &MetricReporter{
		reportService: reportService,

		clusterArns: opts.ClusterArns,
		state:       opts.State,
		startDate:   opts.StartDate,
		endDate:     opts.EndDate,
	}
}

func (r *MetricReporter) Run() error {
	slog.Info("ðŸ” processing clusters", "clusters", r.clusterArns, "startDate", r.startDate, "endDate", r.endDate)

	processedState := r.reportService.ProcessState(*r.state)
	processedClusterMetrics := []types.ProcessedClusterMetrics{}

	// find the clusters in the state

	for _, clusterArn := range r.clusterArns {
		clusterMetrics, err := r.reportService.FilterClusterMetrics(processedState, clusterArn, r.startDate, r.endDate)
		if err != nil {
			return fmt.Errorf("failed to filter cluster metrics: %v", err)
		}
		processedClusterMetrics = append(processedClusterMetrics, *clusterMetrics)
	}

	fileName := fmt.Sprintf("metric_report_%s.md", time.Now().Format("2006-01-02_15-04-05"))
	markdownReport := r.generateReport(processedClusterMetrics)
	if err := markdownReport.Print(markdown.PrintOptions{ToTerminal: false, ToFile: fileName}); err != nil {
		return fmt.Errorf("failed to write markdown report: %v", err)
	}

	return nil
}

func (r *MetricReporter) generateReport(processedClusterMetrics []types.ProcessedClusterMetrics) *markdown.Markdown {
	md := markdown.New()

	md.AddHeading("AWS Metrics Report", 1)
	// Add build info metadata for diagnostics in smaller italic text
	md.AddParagraph(fmt.Sprintf("*Generated by kcp (version: %s, commit: %s, built: %s)*",
		build_info.Version,
		build_info.Commit,
		build_info.Date))
	md.AddParagraph("")

	md.AddParagraph(fmt.Sprintf("**Report Period:** %s to %s",
		r.startDate.Format("2006-01-02"),
		r.endDate.Format("2006-01-02")))

	regionGroups := r.groupClustersByRegion(processedClusterMetrics)

	regions := make([]string, 0, len(regionGroups))
	for region := range regionGroups {
		regions = append(regions, region)
	}
	md.AddParagraph(fmt.Sprintf("**Regions:** %v", regions))

	if len(processedClusterMetrics) > 0 {
		metadata := processedClusterMetrics[0].Metadata
		md.AddParagraph(fmt.Sprintf("**Cluster Type:** %s", metadata.ClusterType))
		md.AddParagraph(fmt.Sprintf("**Kafka Version:** %s", metadata.KafkaVersion))
		md.AddParagraph(fmt.Sprintf("**Enhanced Monitoring:** %s", metadata.EnhancedMonitoring))
		md.AddParagraph(fmt.Sprintf("**Period:** %d seconds", metadata.Period))
	}

	md.AddHorizontalRule()

	for region, clusters := range regionGroups {
		r.addRegionSection(md, region, clusters)
	}

	return md
}

func (r *MetricReporter) groupClustersByRegion(processedClusterMetrics []types.ProcessedClusterMetrics) map[string][]types.ProcessedClusterMetrics {
	regionGroups := make(map[string][]types.ProcessedClusterMetrics)

	for i, clusterMetrics := range processedClusterMetrics {
		// Extract region from cluster ARN
		region := r.extractRegionFromArn(r.clusterArns[i])
		regionGroups[region] = append(regionGroups[region], clusterMetrics)
	}

	return regionGroups
}

func (r *MetricReporter) extractRegionFromArn(arn string) string {
	// ARN format: arn:aws:kafka:region:account:cluster/cluster-name/uuid
	// Split on ':' and take index 3 for region
	parts := strings.Split(arn, ":")
	if len(parts) >= 4 {
		return parts[3]
	}
	return "unknown-region"
}

func (r *MetricReporter) addRegionSection(md *markdown.Markdown, region string, clusters []types.ProcessedClusterMetrics) {
	md.AddHeading(fmt.Sprintf("Region: %s", region), 2)

	// Process each cluster in this region
	for i, clusterMetrics := range clusters {
		if i > 0 {
			md.AddParagraph("---")
		}
		r.addClusterSection(md, clusterMetrics, region)
	}

	md.AddHorizontalRule()
}

func (r *MetricReporter) addClusterSection(md *markdown.Markdown, clusterMetrics types.ProcessedClusterMetrics, region string) {
	// Extract cluster name from ARN - we need to find the matching ARN for this region
	clusterName := r.extractClusterNameFromRegion(region)

	md.AddHeading(fmt.Sprintf("Cluster: %s", clusterName), 3)

	// Add metric aggregates
	if len(clusterMetrics.Aggregates) > 0 {
		md.AddHeading("Metric Aggregates", 4)

		headers := []string{"Metric", "Average", "Maximum", "Minimum"}
		var tableData [][]string

		for metricName, aggregate := range clusterMetrics.Aggregates {
			row := []string{
				metricName,
				r.formatMetricValue(aggregate.Average),
				r.formatMetricValue(aggregate.Maximum),
				r.formatMetricValue(aggregate.Minimum),
			}
			tableData = append(tableData, row)
		}

		if len(tableData) > 0 {
			md.AddTable(headers, tableData)
		}
	} else {
		md.AddParagraph("*No metric aggregates available for this cluster.*")
	}

	// Add individual metric values
	r.addIndividualMetricsSection(md, clusterMetrics.Metrics)
}

func (r *MetricReporter) extractClusterNameFromRegion(region string) string {
	// Find the first ARN that matches this region
	for _, arn := range r.clusterArns {
		if r.extractRegionFromArn(arn) == region {
			return utils.ExtractClusterNameFromArn(arn)
		}
	}
	return "Unknown"
}

func (r *MetricReporter) addIndividualMetricsSection(md *markdown.Markdown, metrics []types.ProcessedMetric) {
	if len(metrics) == 0 {
		md.AddParagraph("*No individual metric data available for this cluster.*")
		return
	}

	md.AddHeading("Individual Metric Values", 4)

	// Group metrics by label (metric type)
	metricGroups := r.groupMetricsByLabel(metrics)

	for metricLabel, metricValues := range metricGroups {
		md.AddHeading(metricLabel, 5)

		if len(metricValues) == 0 {
			md.AddParagraph("*No data points available for this metric.*")
			continue
		}

		// Create table for this metric type
		headers := []string{"Start Time", "End Time", "Value"}
		var tableData [][]string

		for _, metric := range metricValues {
			startTime := r.formatTimestamp(metric.Start)
			endTime := r.formatTimestamp(metric.End)
			value := r.formatMetricValue(metric.Value)

			row := []string{startTime, endTime, value}
			tableData = append(tableData, row)
		}

		if len(tableData) > 0 {
			md.AddTable(headers, tableData)
		}

		// Add some spacing between metric types
		md.AddParagraph("")
	}
}

func (r *MetricReporter) groupMetricsByLabel(metrics []types.ProcessedMetric) map[string][]types.ProcessedMetric {
	groups := make(map[string][]types.ProcessedMetric)

	for _, metric := range metrics {
		groups[metric.Label] = append(groups[metric.Label], metric)
	}

	return groups
}

func (r *MetricReporter) formatTimestamp(timestamp string) string {
	if timestamp == "" {
		return "N/A"
	}

	// Parse the timestamp and format it in a more readable way
	if parsedTime, err := time.Parse("2006-01-02T15:04:05Z", timestamp); err == nil {
		return parsedTime.Format("2006-01-02 15:04:05")
	}

	// If parsing fails, return the original timestamp
	return timestamp
}

func (r *MetricReporter) formatMetricValue(value *float64) string {
	if value == nil {
		return "N/A"
	}
	return fmt.Sprintf("%.2f", *value)
}
