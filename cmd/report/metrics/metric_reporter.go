package metrics

import (
	"fmt"
	"log/slog"
	"time"

	"github.com/confluentinc/kcp/internal/build_info"
	"github.com/confluentinc/kcp/internal/services/markdown"
	"github.com/confluentinc/kcp/internal/types"
	"github.com/confluentinc/kcp/internal/utils"
)

type ReportService interface {
	ProcessState(state types.State) types.ProcessedState
	FilterClusterMetrics(processedState types.ProcessedState, clusterArn string, startTime, endTime *time.Time) (*types.ProcessedClusterMetrics, error)
}

type MetricReporterOpts struct {
	ClusterArns []string
	State       *types.State
	StartDate   *time.Time
	EndDate     *time.Time
}

type MetricReporter struct {
	reportService ReportService

	clusterArns []string
	state       *types.State
	startDate   *time.Time
	endDate     *time.Time
}

func NewMetricReporter(reportService ReportService, opts MetricReporterOpts) *MetricReporter {
	return &MetricReporter{
		reportService: reportService,

		clusterArns: opts.ClusterArns,
		state:       opts.State,
		startDate:   opts.StartDate,
		endDate:     opts.EndDate,
	}
}

func (r *MetricReporter) Run() error {
	slog.Info("ðŸ” processing clusters", "clusters", r.clusterArns, "startDate", r.startDate, "endDate", r.endDate)

	processedState := r.reportService.ProcessState(*r.state)
	processedClusterMetrics :=  []types.ProcessedClusterMetrics{}

	// find the clusters in the state

	for _, clusterArn := range r.clusterArns {
		clusterMetrics, err := r.reportService.FilterClusterMetrics(processedState, clusterArn, r.startDate, r.endDate)
		if err != nil {
			return fmt.Errorf("failed to filter cluster metrics: %v", err)
		}
		processedClusterMetrics = append(processedClusterMetrics, *clusterMetrics)
	}

	fileName := fmt.Sprintf("metric_report_%s.md", time.Now().Format("2006-01-02_15-04-05"))
	markdownReport := r.generateReport(processedClusterMetrics)
	if err := markdownReport.Print(markdown.PrintOptions{ToTerminal: false, ToFile: fileName}); err != nil {
		return fmt.Errorf("failed to write markdown report: %v", err)
	}

	return nil
}

func (r *MetricReporter) generateReport(clusters []types.ProcessedClusterMetrics) *markdown.Markdown {
	md := markdown.New()

	md.AddHeading("AWS Metrics Report", 1)
	// Add build info metadata for diagnostics in smaller italic text
	md.AddParagraph(fmt.Sprintf("*Generated by kcp (version: %s, commit: %s, built: %s)*",
		build_info.Version,
		build_info.Commit,
		build_info.Date))
	md.AddParagraph("")

	md.AddParagraph(fmt.Sprintf("**Report Period:** %s to %s",
		r.startDate.Format("2006-01-02"),
		r.endDate.Format("2006-01-02")))

	if len(r.clusterArns) > 0 {
		md.AddParagraph("**Clusters included in report:**")
		for _, arn := range r.clusterArns {
			md.AddParagraph(fmt.Sprintf("- %s", arn))
		}
	}

	md.AddHorizontalRule()

	// Process each cluster in this region
	for i, clusterMetrics := range clusters {

		if i > 0 {
			md.AddHorizontalRule()
		}
		r.addClusterSection(md, clusterMetrics)
	}

	md.AddHorizontalRule()

	return md
}


func (r *MetricReporter) addClusterSection(md *markdown.Markdown, clusterMetrics types.ProcessedClusterMetrics) {
	// Extract cluster name from ARN - we need to find the matching ARN for this region


	md.AddHeading(fmt.Sprintf("Cluster Name: %s", utils.ExtractClusterNameFromArn(clusterMetrics.ClusterArn)), 3)
	md.AddParagraph(fmt.Sprintf("**Cluster ARN**: %s", clusterMetrics.ClusterArn))
	md.AddParagraph(fmt.Sprintf("**Region**: %s", clusterMetrics.Region))
	md.AddParagraph(fmt.Sprintf("**Cluster Type**: %s", clusterMetrics.Metadata.ClusterType))
	md.AddParagraph(fmt.Sprintf("**Number of Broker Nodes**: %d", clusterMetrics.Metadata.NumberOfBrokerNodes))
	md.AddParagraph(fmt.Sprintf("**Kafka Version**: %s", clusterMetrics.Metadata.KafkaVersion))
	md.AddParagraph(fmt.Sprintf("**Enhanced Monitoring**: %s", clusterMetrics.Metadata.EnhancedMonitoring))
	md.AddParagraph(fmt.Sprintf("**Period**: %d seconds", clusterMetrics.Metadata.Period))
	md.AddParagraph(fmt.Sprintf("**Follower Fetching**: %t", clusterMetrics.Metadata.FollowerFetching))
	md.AddParagraph(fmt.Sprintf("**Instance Type**: %s", clusterMetrics.Metadata.InstanceType))
	md.AddParagraph(fmt.Sprintf("**Tiered Storage**: %t", clusterMetrics.Metadata.TieredStorage))

	// Add metric aggregates
	if len(clusterMetrics.Aggregates) > 0 {
		md.AddHeading("Metric Aggregates", 4)

		headers := []string{"Metric", "Average", "Maximum", "Minimum"}
		var tableData [][]string

		for metricName, aggregate := range clusterMetrics.Aggregates {
			row := []string{
				metricName,
				r.formatMetricValue(aggregate.Average),
				r.formatMetricValue(aggregate.Maximum),
				r.formatMetricValue(aggregate.Minimum),
			}
			tableData = append(tableData, row)
		}

		if len(tableData) > 0 {
			md.AddTable(headers, tableData)
		}
	} else {
		md.AddParagraph("*No metric aggregates available for this cluster.*")
	}

	// Add individual metric values
	r.addIndividualMetricsSection(md, clusterMetrics.Metrics)
}

func (r *MetricReporter) addIndividualMetricsSection(md *markdown.Markdown, metrics []types.ProcessedMetric) {
	if len(metrics) == 0 {
		md.AddParagraph("*No individual metric data available for this cluster.*")
		return
	}

	md.AddHeading("Individual Metric Values", 4)

	// Group metrics by label (metric type)
	metricGroups := r.groupMetricsByLabel(metrics)

	for metricLabel, metricValues := range metricGroups {
		md.AddHeading(metricLabel, 5)

		if len(metricValues) == 0 {
			md.AddParagraph("*No data points available for this metric.*")
			continue
		}

		// Create table for this metric type
		headers := []string{"Start Time", "End Time", "Value"}
		var tableData [][]string

		for _, metric := range metricValues {
			startTime := r.formatTimestamp(metric.Start)
			endTime := r.formatTimestamp(metric.End)
			value := r.formatMetricValue(metric.Value)

			row := []string{startTime, endTime, value}
			tableData = append(tableData, row)
		}

		if len(tableData) > 0 {
			md.AddTable(headers, tableData)
		}

		// Add some spacing between metric types
		md.AddParagraph("")
	}
}

func (r *MetricReporter) groupMetricsByLabel(metrics []types.ProcessedMetric) map[string][]types.ProcessedMetric {
	groups := make(map[string][]types.ProcessedMetric)

	for _, metric := range metrics {
		groups[metric.Label] = append(groups[metric.Label], metric)
	}

	return groups
}

func (r *MetricReporter) formatTimestamp(timestamp string) string {
	if timestamp == "" {
		return "N/A"
	}

	// Parse the timestamp and format it in a more readable way
	if parsedTime, err := time.Parse("2006-01-02T15:04:05Z", timestamp); err == nil {
		return parsedTime.Format("2006-01-02 15:04:05")
	}

	// If parsing fails, return the original timestamp
	return timestamp
}

func (r *MetricReporter) formatMetricValue(value *float64) string {
	if value == nil {
		return "N/A"
	}
	return fmt.Sprintf("%.2f", *value)
}
